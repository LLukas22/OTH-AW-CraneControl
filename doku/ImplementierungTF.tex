\section{Tensorflow - Python}
\subsection{Konzept}
Es wird ein Object-Detection-Model verwendet um über eine Webcam eine von sechs Gesten zu erkennen.
Die erkannte Geste wird in einem Buffer zwischengespeichert, und wenn mehrere Durchläufe das gleiche Ergebniss liefern wird ein Befehl an den Kran gesendet.

\begin{figure}[H]
    \centering
    \subfigure[Power]{\includegraphics[width=0.3\textwidth]{TensorFlow/Gestures/Power.PNG}}
    \subfigure[Up]{\includegraphics[width=0.3\textwidth]{TensorFlow/Gestures/Up.PNG}}
    \subfigure[Down]{\includegraphics[width=0.3\textwidth]{TensorFlow/Gestures/Down.PNG}}
    \subfigure[Left]{\includegraphics[width=0.3\textwidth]{TensorFlow/Gestures/Left.PNG}}
    \subfigure[Right]{\includegraphics[width=0.3\textwidth]{TensorFlow/Gestures/Right.PNG}}
    \subfigure[Toggle]{\includegraphics[width=0.3\textwidth]{TensorFlow/Gestures/Toggle.PNG}}
    \caption[Tensorflow Gesten]{Tensorflow Gesten. Bildquelle: eigene Bilder}
    \label{fig:Tensorflow Gesten}
\end{figure}
\newpage
\subsection{Tensorflow Trainer}
Um das trainieren der Modele zu erleichtern wurde eine C\# Application geschrieben, die diesen Vorgang weitgehend automatisiert.

\begin{figure}[H]
    \centering
    \subfigure{\includegraphics[width=0.9\textwidth]{TensorFlow/TensorflowTrainer.PNG}}
    \caption[Tensorflow Trainer]{Tensorflow Trainer}
    \label{fig:Tensorflow Trainer}
\end{figure}

\subsection{Trainieren eines eigene Models}

\subsubsection{Voraussetzungen:}

\begin{itemize}
    \item Git (\url{https://git-scm.com/downloads})
    \item C++ Build Tools (\url{https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=BuildTools&rel=16})
    \item Python 3.7.X or newer (\url{https://www.python.org/downloads/})
    \item LabelImg (\url{https://github.com/tzutalin/labelImg})
    \item Nvidia Gpu (optional but highly recommended)
          \begin{itemize}
              \item CUDA Version (\url{https://www.tensorflow.org/install/gpu})
              \item cuDNN Version (\url{https://developer.nvidia.com/cudnn})
          \end{itemize}
\end{itemize}
\newpage
\subsubsection{Erstellen eines Datensatzes}
Um eine Datensatz zu erzeugen müssen zunächst mit LabelImg Boxen (XML PASCAL VOC Format) um die zu Erkennenden Objekte gezogen werden.
Hierbei sind 150-250 Bilder pro Objekt ein guter Richtwert.

\subsubsection{Python Packete}
Die benötigten Pip-Packete werden durch Klicken des \textit{Install}-Buttons im TensorflowTrainer installiert. Soll die GPU-Version von Tensorflow installiert werden muss die CheckBox  \textit{GPU Acceleration} abgehackt werden.
Der  \textit{Uninstall}-Button entfernt alle installierten Packete.

\subsubsection{Settings.json}
Nach erstem öffnen und schließen des TensorflowTrainer wird eine Settings.json erzeugt.
\lstinputlisting[language=json,label={lst:Settings.json},
    numbers={none},
    caption={Settings.json}]
{CodeSamples/Tensorflow/Settings.json}
Hier können Einstellungen angepasst werden, wobei das Feld \textbf{ModelUrl} das wichtigste ist, da hier das zu verwendende Model des TF-Model-Zoo (\url{https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md}) angegeben werden kann.

\subsubsection{Downloads und Setup}
Um die Restlichen Daten herunterzuladen, müssen die drei \textit{Download}-Buttons betätigt werden. Um die Downloads zu löschen kann der \textit{Clear Cache}-Button verwendet werden.
Nun muss in Environment ein Ordner augewähl werden, in dem das Training statfinden soll. Danach muss der \textit{Build}-Button betätigt werden. Der \textit{Open}-Button öffnet den ObjectDetection-Ordner.
Um die Installation abzuschließen muss zuerst der \textit{Compile *.protoc Files}- und danach der \textit{Execute setup.py}-Button betätigt werden. Nun werden alle ObjectDetection Module installiert und die Tensorflow Tests ausgeführt.
Wenn das Testergebnis \textit{OK} ist war die Installation erfolgreich.

\subsubsection{TF Records}
Als Nächstes müssen alle Bilder mit *.Xml Dateien nach ObjectDetection/images/input kopiert werden und die in LabelImg verwendeten Labels in richtiger Reihenfolgen in die Textbox eingetragen werden.
Danach die 4 Buttons von Links nach Rechts betätigen. Dies erzeugt test- und train.record im ObjectDetection-Ordner und autogeneriert die Labelmap und customPipeline in ObjectDetection/training.

\subsubsection{Pipeline Config}
In der customPipeline.config können nach bedarf DataAugemntation-Optionen hinzugefügt oder entfernt werden (\url{https://stackoverflow.com/questions/44906317/what-are-possible-values-for-data-augmentation-options-in-the-tensorflow-object}).
\lstinputlisting[language=json,label={lst:Pipeline},
    numbers={none},
    caption={Pipeline},
    firstline=137,
    lastline=153]
{CodeSamples/Tensorflow/customPipeline.config}

Die wichtigsten anpassbaren Werten sind:

\begin{itemize}
    \item \textbf{Batch_Size} was die auf einmal zu verarbeitenden Bilder sind. Falls nicht genug RAM im System verfügbar ist muss die Batch_Size gesenk werden.
    \item \textbf{learning_rate_base} Geht der Loss nach einigen Schritten gegen Unendlich ist die LearningRate zu hoch.
    \item \textbf{warmup_learning_rate} ähnlich wie learning_rate_base gilt aber nur für ersten warmup_steps.
\end{itemize}


\subsubsection{Training}
Das Training wird durch eine Click auf \textit{Start Training!} gestartet. Als andere Option kann man auch im ObjectDetection Ordner \textit{python model_main_tf2.py} ausführen. Tensorboard kann über einen Click auf \textit{Open Tensorboard} geöffnet werden.
Ändert der Loss sich nach einiger Zeit nicht mehr kann das Training abgebrochen werden und das fertige Model über einen Klick auf \textit{Export Graph} nach ObjectDetection/export/normal exportiert werden und über  \textit{Test on Webcam} getestet werden.
\\
\textbf{Anmerkungen}
\begin{itemize}
    \item[1.] Falls die Gpu nicht über genug Videospeicher verfügt besteht die Option Systemspeicher als Puffer zu verwednen. Dazu müssen in \textit{object_detection/model_main_tf2.py} nach dem Tensorflow import folgende Zeilen eingefügt werden:
          \lstinputlisting[language=python,label={lst:Gpu Growth},
              numbers={none},
              caption={Gpu Growth},
              firstline=30,
              lastline=36]
          {CodeSamples/Tensorflow/model_main_tf2.py}
    \item[2.] Falls der Trainings Prozess einen Evaluations Durchgang ausführt und der TensorflowTrainer versucht den Prozess zu schließen läuft der Pyton Prozess weiter im Hintergrund -> Schließen über TaskManager.

\end{itemize}

\subsubsection{Lite Graph}
Mit einem Klick auf \textit{Export Lite Convertible Graph} wird ein Saved Model nach ObjectDetection/export/LiteConvertibleGraph exportiert, das mit dem TFLite-Converter (\url{https://www.tensorflow.org/lite/convert}) zu einem TFLite Model convertiert werden kann.
\newpage
\subsection{Python Anwendung}
Um die trainierten Modelle verwenden zu können wurde eine Python Anwendung geschrieben, die TF 1, TF 2 und TF Lite Modelle unterstützt.
\begin{figure}[H]
    \centering
    \subfigure{\includegraphics[width=0.9\textwidth]{TensorFlow/ApplicationArchitecture.png}}
    \caption[Architektur]{Architektur}
    \label{fig:Tensorflow Architektur}
\end{figure}
Die Anwendung wurde für den Raspberry Pi 4 optimiert und verwendet alle 4 Prozessor Kerne gleichzeitg, dennoch sollte beachtet werden, dass die Rechenleistung des PI sehr beschränkt ist und nur TF Lite Modelle verwendet werden sollten.
\newpage
\subsubsection{GUI}
Die GUI wurde sehr simpel gehalten um möglichst wenig CPU-Time zu beanspruchen.
\begin{figure}[H]
    \centering
    \subfigure{\includegraphics[width=1\textwidth]{TensorFlow/GUI.PNG}}
    \caption[Pyton Gui]{Pyton Gui}
    \label{fig:Tensorflow GUI}
\end{figure}
Mit den \textit{Stop} und  \textit{Restart} Buttons können die einzelnen Threads des Programms angehalten oder neu gestartet werden. In der Linken oberen Ecke sieht man die derzeitige CPU Auslastung und die letzte Tensorflow-Detection Zeit. Wenn man den Tensorflow Thread angehalten hat ist es Möglich den Kran direkt über die Richtungs Buttons zu steuern.

\subsubsection{Einstellungen}
Die Einstellungen können über Variablen in \textit{CraneControl.py} angepasst werden.

\lstinputlisting[language=python,label={lst:CraneControl Settings},
    numbers={none},
    caption={CraneControl Settings},
    firstline=24,
    lastline=40]
{CodeSamples/Tensorflow/CraneControl.py}


\subsubsection{Setup und Ausführung}
Die Anwendung wird durch Aufrufen von \textit{CraneControl.py} mit Python gestartet.
Benötiget pip Packete: Pillow,opencv-python,psutil,tensorflow (nur x86/x64)

\textbf{Für PI:} \\
Um die Anwendung auf einem PI verwenden zu können muss der TensorFlow Lite-Interpreter installiert werden(\url{https://www.tensorflow.org/lite/guide/python}).
Außerdem müssen einige APT Packete installiert werden. (python3-pil python3-pil.imagetk libatlas-base-dev libhdf5-dev libhdf5-serial-dev libatlas-base-dev libjasper-dev libqtgui4 libqt4-test).
Auf dem Pi können nur Lite-Graphen ausgeführt werden.


\textbf{Mitgelieferte Modelle:}
\begin{itemize}
    \item mobilnet_300x300v2.tflite [TFLite] (Ungenau aber sehr schnell)
    \item mobilnet_300x300.pb [TF1] (Genau aber schnell)
    \item ResNet 50 [TF2] (Sehr genau aber langsam)
\end{itemize}
